# Future Research Directions

While our multi-class authorship attribution system successfully distinguishes between human-written, AI-generated, and machine-translated text, several promising avenues remain unexplored. Future work should investigate cross-domain generalization by testing the model on non-political text (literature, technical writing, social media) to validate whether learned features capture authorship rather than topic. Additionally, extending the framework to include finer-grained distinctions—such as differentiating between specific LLM families (GPT-4 vs. Claude vs. Llama) or identifying paraphrased AI text—would provide more granular detection capabilities. Incorporating temporal analysis to detect evolving AI writing styles as models improve, and deploying the system in real-world applications (academic integrity tools, journalism verification) with human-in-the-loop feedback, represents the natural progression toward practical deployment. Finally, adversarial robustness testing against sophisticated evasion techniques (iterative paraphrasing, human post-editing of AI text) will be critical for ensuring the system's reliability in adversarial environments.
