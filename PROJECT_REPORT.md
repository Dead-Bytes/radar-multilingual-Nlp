# Project Report: Multi-Class Authorship Attribution

**Author:** deadbytes
**Date:** 2025-11-08
**Version:** 1.0

## 1. Problem Statement (The "Why")

The current state-of-the-art in AI-generated text detection is predominantly focused on **binary classification**: determining if a text is written by a `Human` or an `AI`. This approach, while valuable, has a significant blind spot. It fails to account for the diverse nature of machine-generated text, treating output from a creative Large Language Model (LLM) the same as output from a Neural Machine Translation (NMT) service.

Our research, as outlined in the paper "Understanding and Improving Limitations of Multilingual AI Text Detection," identifies this as the **"Translator as a Generator"** problem. Lumping all machine-generated text into a single "AI" category prevents a nuanced understanding of authorship and can lead to incorrect conclusions.

This project directly addresses this limitation. Our goal is to develop a **multi-class authorship attribution system** capable of distinguishing between at least three distinct sources:
1.  **Human-Written Text:** Authored directly by a person.
2.  **AI-Generated Text:** Creatively generated by an LLM.
3.  **Machine-Translated Text:** Translated from one language to another by an NMT model.

## 2. Methodology & Implementation (The "How")

To achieve this goal, we have completed the foundational data engineering and model scaffolding steps.

### 2.1. Phase 1: Multi-Class Dataset Creation

A multi-class model requires a multi-class dataset. We created a proof-of-concept dataset by leveraging existing project assets.

*   **Script Developed:** `prepare_data.py`
*   **Process:**
    1.  **Load Source Data:** The script loads `multitude.csv` (containing human and AI-generated English text) and `multitude_portuguese_translated.csv` (containing machine-translated Portuguese text).
    2.  **Extract Classes:**
        *   **`human`:** Samples where `label == 0` and `language == 'en'` are extracted from `multitude.csv`.
        *   **`ai_generated`:** Samples where `label == 1` and `language == 'en'` are extracted from `multitude.csv`.
        *   **`machine_translated`:** All text samples are extracted from `multitude_portuguese_translated.csv`, as the act of translation is the primary feature for this class.
    3.  **Combine and Label:** A balanced set of 500 samples from each class was combined into a single dataset. A new `source_label` column was added to serve as our target variable for training.
*   **Output Artifact:** `processed_xai_dataset.csv`. This file contains 1,500 text samples, each labeled as `human`, `ai_generated`, or `machine_translated`.

### 2.2. Phase 2: Training Pipeline Scaffolding

With the dataset ready, we prepared the script for training our multi-class classifier.

*   **Script Developed:** `train_multiclass.py`
*   **Process:**
    1.  **Data Loading:** The script is configured to load our newly created `processed_xai_dataset.csv`.
    2.  **Model Selection:** We selected `microsoft/mdeberta-v3-base`, a powerful transformer model with strong multilingual capabilities, making it well-suited for this task.
    3.  **Model Configuration:** The script uses the Hugging Face `transformers` library to load the model and its tokenizer. Crucially, it configures the model for multi-class classification by setting `num_labels=3`.
    4.  **Training Setup:** The script initializes the `Trainer` class with the model, tokenized datasets, and placeholder training arguments.

## 3. Importance of This Approach

This methodology is crucial for any research project for several key reasons:

*   **Reproducibility:** By scripting the data preparation (`prepare_data.py`), we ensure that the dataset can be recreated perfectly every time. This is the cornerstone of reproducible research.
*   **Clarity and Modularity:** Separating data preparation from model training (`prepare_data.py` vs. `train_multiclass.py`) makes the project easier to understand, debug, and extend.
*   **Audit Trail:** These reports, along with version-controlled scripts, create a clear audit trail of what was done, why it was done, and how it was implemented. This is invaluable for writing papers, collaborating with others, and onboarding new team members.

## 4. Next Steps

The foundation is now laid. The immediate next steps are:
1.  **Train the Model:** Execute the `train_multiclass.py` script to fine-tune the `mDeBERTa` model on our new dataset.
2.  **Evaluate Performance:** Analyze the model's accuracy, precision, recall, and F1-score for each of the three classes. A confusion matrix will be essential here.
3.  **Implement Explainability (XAI):** Integrate techniques like SHAP or LIME to investigate *which* textual features the model is using to differentiate between human, AI, and translated text.
